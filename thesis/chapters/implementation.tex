\chapter{Implementation}

Path tracing is expensive because path construction requires tracing multiple rays in a scene.
One repeats this procedure for all pixels of an image.
Elaborating on the idea of tracing rays, one utilises ray casts to send a ray into a scene. 
Based on that, one can find an object with the corresponding intersection point.
For that, one employs intersection tests against the present geometry of objects in the scene description.

The challenge with tracing many rays is that a computer must process these calculations as fast and efficient as possible.
For example, if an image has the dimension 1024x1024 and one samples only one path per pixel, then there are already a million paths to be calculated. 
Depending on the path length, the algorithm has to find $n$ intersection point.
Extending the problem with a more complex scene description or increasing light calculations per intersection point, the effort to calculate a path becomes much more problematic.

Accordingly, one must consider two factors to achieve a foreseeable time for image synthesis.
The first point is ray tracing libraries that offer required acceleration structures, efficient algorithms for ray tracing, and a convenient interface.
Secondly, one should consider the aspect of parallelisation.

\section{Ray Tracing Libraries}

First, one likes to take a closer look at ray tracing libraries.
All the present libraries follow the same core principle, providing primary ray tracing primitives like acceleration structures and ray tracing itself with a programmable pipeline.
The programmable components of these pipelines are shader programs that govern the hit and miss logic for graphic units or abstract classes and lambda functions for CPU implementations.
Important to note is that GPU libraries tend to be more verbose, demanding more explicit source code regarding general data management.

The second topic is parallel computing.
It has become an essential field in computer graphics over the last decades as hardware performance does not follow Moore's law anymore; \cite{theis_end_2017}.
Consequently, experts adapted software design and introduced specialised hardware components.
Accordingly, CPUs can process multiple serial tasks owing to an increase in core count and cache, enabling "lightweight" parallelism.
Nonetheless, GPUs as specialised hardware became popular for massive parallelism, especially in computer graphics.
In recent years, graphics card vendors extended the architecture for machine learning and ray tracing support, making GPUs very interesting for path tracer implementation.

Before diving into the concrete tools, one will specify some requirements.
The first objective is fast rendering so one can iterate through changes frequently.
Fast rendering is beneficial if one chooses high sample counts to check for potential image artefacts like fireflies, as images should converge correctly.
Secondly, the used library should offer high abstraction so that no technical intricacies of the library lead to problems and therefore accelerate the development speed.
The last requirement is libraries must support C++ because of suitable tools and resources.
Based on this, one can try to make a decision.

First, one wants to determine whether to use the graphics cards or the processor for the implementation.
The performance and rendering speed depends solely on the library and possible parallel computing.
Therefore, the premise is that the library's choice is not decisive because the algorithms and data structures can be virtually equivalent \cite{bico_optix_2016}.
Hence, with simplified conditions, only the parallelisation provided by the corresponding hardware would determine the final runtime. 
Due to a certain degree of guaranteed parallelism that a graphics card provides, one uses a corresponding GPU library for the implementation.

\begin{wraptable}{l}{6cm}
    \begin{tabular}{|c|c|}\hline
        Nvidia        & OptiX     \\ \hline
        Microsoft     & DXR       \\ \hline
        Khronos Group & Vulkan-RT \\ \hline
        Intel         & Embree    \\ \hline
    \end{tabular}
    \caption{libraries}
    \label{fig:libraries}
\end{wraptable}

One further examined selected libraries for a graphics card implementation (see fig.~\ref{fig:libraries}).
The pre-selection of the four libraries is because of a mature ray tracing API with various potential features to enable fast ray tracing.

The essential requirement for one is a straightforward interface so that rapid development is favoured.
However, as already mentioned, the interfaces are often verbose, and the libraries expect prior knowledge of graphics card programming.
Apart from that, there were no other expectations or limitations.
Therefore, one favoured a wrapper library that streamlines the corresponding parts, such as function calls or data management.
A suitable library, OptiX Wrapper Library (OWL), was found, which led to the decision to use OptiX. 

Finally, one would like to list the tools and libraries used.
It will be not explained in more detail why one used the respective versions or tools.

\begin{itemize}
    \item CMake 3.22.0
    \item C++20 and CUDA 17
    \item CUDA Runtime 11.7
    \item OptiX Ray Tracing API (SDK 7.4.0)
\end{itemize}

\section{Nvidia OptiX 7}

Building a path tracer with a ray tracing library requires knowing some terminology and understanding the various parts of the programmable pipeline.
OptiX is a part of the CUDA programming eco-system and differentiates between host and device code.
In simple words, host code runs on the processor and device code on a graphics card.
Accordingly, any source code targeted to the graphics card must be device code and follow the CUDA language specifications.

In the context of OptiX, the programmable parts are shaders or \textit{programs} that are a part of the device code.
These programs define specific behaviour for different states in this pipeline provided by the programmer and depending on the application.
Next, one will briefly explain which program types exist and how one uses these programs.

\begin{figure}[h]
    \begin{center}
        \includegraphics[width=\textwidth]{optix-kernel.png}
    \end{center}
    \caption{optix pipeline}
    \label{fig:optix-kernel}
\end{figure}

The \textit{Ray Generation} program is the entry point of the device execution.
One uses a particular host code function to initiate the device-sided execution.
From here on, one can call additional functions to build an execution sequence.

A \textit{Intersection} and \textit{Bounds} program are necessary for the acceleration structure.
If one calls the trace function, it iterates through the data structure until the routine reaches a leaf.
For custom geometric primitives, one must specify the bounds and intersection programs so that OptiX knows how to process them during iteration.
The only exceptions are triangles and curves since they are already predefined.
OWL forms, on top of it, an abstraction for minimum expenditure.

The last two required programs are the \textit{Closest Hit} and \textit{Miss} program.
Both describe a case each, whether a ray has hit an object or not.
The execution of these programs starts after a complete iteration through the acceleration structure. 
After the execution, the application returns to the trace function call.
The only distinction between the two is that the data from the hit object is available in the closest hit program.

\section{Application Design}

This section intends to give a general overview of the application.
It describes the individual tasks of the application.
One will walk through some essential points that shaped the application design.
Note that the decision was based on simplicity and not necessarily robust and flexible design.

Let one start with a host-only functionality, which is loading mesh and material data.
Three-dimensional meshes have different representations and need corresponding loaders to bring the vertex normal and position data to memory.
Besides that, these representations can also contain information about the material data.
Accordingly, large libraries can load any file format for objects.
However, this seemed to be convoluted, and thus one decided to use an OBJ loader because of its simplicity. 
Another argument for the OBJ format was that all object vertices are in global coordinates; if needed, one could also load the normals and UV data.
One described the materials of the objects via a JSON format.

The next consideration is the exchange of data and where the data should be made available.
OptiX offers for the exchange of data two strategies.
The first is the so-called SBT or shader binding table.
They allow the programmer to bind specific data to a program of the OptiX pipeline.
An essential difference to the second option is that the data will be only available to the designated program.
The second option is to create global constants that make the data available everywhere in the device.
However, the only important attribute is simplicity and creating a global constant containing any data required during the renderer's lifetime is the easiest choice.

The next point is the path tracing itself, especially if one selects a recursive or iterative approach for path tracing.
Depending on the chosen strategy, the tasks of the programs look different, i.e. the implementation of the path tracer is in \textit{Closest Hit}  or \textit{Ray Generation} program.
Intuitively, a recursive approach would be the most straightforward approach because one described path tracing as a recursive formula.
However, one chooses an iterative version because the path construction is more apparent, and the procedure is easier to understand.
Hence, the implementation will be in the \textit{Ray Generation} program.

One summarises the different decisions and draws the connection to how these interoperate in the application.
The host-sided application loads all required data for the tracer, initialises with OWL the OptiX components and uploads the data to the graphics card.
Afterwards, one launches the path tracing, and the host waits until the result is available through a host device buffer.
The launch causes an invocation of the \textit{Ray Generation} program per pixel, which controls the calculation of paths that have a vertex on the camera plane associated with the corresponding pixel.
The \textit{Closest Hit} and \textit{Miss} programs set the necessary state that allows the tracer to load the mesh data in the path tracing routine.
After the device has finished all calculations, the host can read the result and write it to a predefined destination. 

The last parts of this chapter explain essential aspects of the path tracer in more detail and provide a language-agnostic pseudo-code.

\section{Path Construction Routine}

First, one will discuss the path construction routine in more detail.
The entire pseudo-code can be found in the appendix and should be self-explanatory.
However, one would like to detail the individual parts and explain why one chose this structure.

Foremost, one would like to talk about the state of a path.
The state is essential for determining how much a given path contributes to the outcome.
The three crucial metrics are radiance, throughput and depth.
Radiance is the total energy sent along the path.
Now, one recollects that light reflects or is re-emitted by several materials.
For this reason, one must also take the throughput into account.
Otherwise, a throughput of $1$ would mean that surfaces do not affect light's energy state.
In this specific case, it serves as a coefficient to weigh the radiance and thus indicates the received energy by the camera.
If one considers energy conservation, it is possible to create more robust models for light simulation because less extreme values for the throughput occur.
The last point is the depth of the path.
In general, one can say that paths with a high depth contribute less to the final result than those with short depths.
One can justify this statement with the throughput.
Let one recall the eq.~\ref{eq:path-throughput}. 
The throughput is a product of the individual reflectance values, a cosine of the incoming direction with $\omega_i$ and the value of the density distribution function.
To put this in simpler terms, let's say we have a reflectance of $0.8$ and a radiance of $1$. So, paths with a depth of $2$ contribute more to the final result than those with a depth of $16$.

Next, one will explain the individual steps of the routine.
The initial step is to execute a \texttt{TraceRay} function because the task is to find the subsequent vertex of the path.
Remember the visibility function $V(x \leftrightarrow x')$ eq.~\ref{eq:geometry-term} gives a value of either $0$ or $1$ depending on if two points are visible from each other.
Because one utilises ray tracing, the visibility function always has the value of $1$.
Note that this process is not mandatory, as one can theoretically randomly pick two points in the scene and construct based on that segments of the path.
However, it turns out that ray tracing is the most efficient approach to finding paths. 
Of course, there are extensions to this procedure like bidirectional path tracing that modify the strategy to find paths with another approach.
The following step is to check if the traced ray hits an object.
If it is a miss, one samples the environment's radiance.
Otherwise, one checks if the object is a light, which would also mean sampling the radiance and terminating the construction.
One previously mentioned that one has to handle lights differently, see 
eq.~\ref{eq:path-throughput}.
However, in this case, one found the light through a traced ray from a vertex, which means that the probability density originates from the distribution of solid angles.
Hence, no special care for lights, but if one introduces multi-importance sampling (MIS) and samples from the collection of lights, it would be a PDF over areas.
This fact is crucial to remember because otherwise, one will miscalculate.
The final scenario is a standard surface hit, separated into three parts.
Loading intersection information with all values in global space and mesh data is self-explanatory.
Based on that hit information, one is sampling in local space the material to find the next direction for the ray cast as well as the reflectance and PDF value.
The new direction is in local space, and one can transform it into global space.

One will clarify local and global space; see fig.~\ref{fig:local-space}.

\begin{figure}[h]
    \begin{center}
        \includegraphics[width=0.4\textwidth]{local-space-coordinates.png}
    \end{center}
    \caption{local shading space coordinates}
    \label{fig:local-space}
\end{figure}

One must decide which orthogonal vector basis to utilise for the light calculations.
The first option is the global basis vectors determined by the normal $N_G$, which is already sufficient for the calculations.
However, one has decided to employ local basis vectors with a normal $N_S = (0,0,1)$ for more straightforward and explicit computations.
For example, in the formula, $\cos(\vec{v})=v.z$, the cosine is the Z-component of the vector.

The last step in the path construction is once Russian Roulette.
This function uses throughput and randomness to determine whether the path should end early and thus not contribute to the final result.
Notably, a small throughput should favour termination to reduce calculations for values with a minimal effect on the image's appearance.

The steps repeat until the path terminates because of a miss, light source hit, Russian Roulette or a path depth that equals or exceeds the maximum depth.
Perhaps also worth mentioning is that the maximum path depth intends to prevent paths from being trapped and possibly terminated after many iterations of path construction.
Of course, this problem can only occur if Russian Roulette cannot terminate the path.

\section{Disney Materials}



\subsection*{Sampling Routine}



\subsection*{Evaluation Routines}


